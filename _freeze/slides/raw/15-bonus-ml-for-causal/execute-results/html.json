{
  "hash": "52b01a40c6d5dc8c401b28183722cf98",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Machine Learning for Causal Inference\"\nauthor: \"Malcolm Barrett\"\ninstitute: \"Stanford University\"\nformat: \"kakashi-revealjs\"\n---\n\n\n\n# Machine learning cannot automate causal inference... but maybe it can help some difficult parts of estimating causal effects {background-color=\"#23373B\"}\n\n# Review: Estimands, estimators, and estimates {background-color=\"#23373B\"}\n\n## {background-color=\"#23373B\" .huge .center}\n\nNormal regression estimates associations. But we want *causal* estimates: what would happen if *everyone* in the study were exposed to x vs if *no one* was exposed.\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/estimands-cake.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: tiny\nImage source: Simon Grund\n:::\n\n## What part of the DAG do we want to try to deal with?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## What part of the DAG do we want to try to deal with?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## What part of the DAG do we want to try to deal with?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## Inverse Probability Weighting (IPW) {background-color=\"#23373B\"}\n\n1. Fit a model for `x ~ z` where z is all confounders\n2. Calculate the propensity score for each observation\n3. Calculate the weights\n4. Fit a weighted regression model for `y ~ x` using the weights\n\n## Inverse Probability Weighting (IPW)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## G-computation {background-color=\"#23373B\"}\n\n1. Fit a model for `y ~ x + z` where z is all confounders\n2. Create a duplicate of your data set for each level of `x`\n3. Set the value of x to a single value for each cloned data set (e.g `x = 1` for one, `x = 0` for the other)\n\n\n## G-computation {background-color=\"#23373B\"}\n\n4. Make predictions using the model on the cloned data sets\n5. Calculate the estimate you want, e.g. `mean(x_1) - mean(x_0)`\n\n## G-computation\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## What part of the DAG do we want to try to deal with?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n# Two Causal Questions {background-color=\"#23373B\"}\n\n## **Does quitting smoking cause weight gain?**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Example: The Seven Dwarfs Mine Train {.small}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n<img src=\"img/Mickey_and_Walt_Disney.jpg\" height=\"300\"></img>\n\nPhoto by Anna [CC-BY-SA-4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en)\n:::\n\n::: {.column width=\"50%\"}\nHistorically, guests who stayed in a Walt Disney World resort hotel were able to access the park during \"Extra Magic Hours\" during which the park was closed to all other guests.\n\nThese extra hours could be in the morning or evening.\n\nThe Seven Dwarfs Mine Train is a ride at Walt Disney World's Magic Kingdom. Typically, each day Magic Kingdom may or may not be selected to have these \"Extra Magic Hours\".\n:::\n\n::::\n\n## {background-color=\"#23373B\" .large .center}\n\n*We are interested in examining the relationship between whether there were* **\"Extra Magic Hours\"** *in the morning and* **the average wait time** *for the Seven Dwarfs Mine Train the same day between 9am and 10am.*\n\n---\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n# Machine Learning for Causal Inference {background-color=\"#23373B\"}\n\n## What algorithm should we use to make predictions?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/superlearner.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: tiny\nImage source: Sherri Rose\n:::\n\n## Ensemble Algorithms with SuperLearner\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](img/ml_algorithms.png){fig-align='center' width=80%}\n:::\n:::\n\n\n:::{.fragment}\nGiven a set of candidate algorithms (and hyperparameters), stacked ensembles combine them to minimize (cross-validated) prediction error. Stacked ensembles will perform at least as well as the best individual algorithm.\n:::\n\n## SuperLearner: Exposure Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|1,3,10|4-7|\"}\nsl_library <- c(\"SL.glm\", \"SL.ranger\", \"SL.gam\")\n\npropensity_sl <- SuperLearner(\n  Y = as.integer(nhefs_complete_uc$qsmk == \"Yes\"),\n  X = nhefs_complete_uc |>\n    select(sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n    mutate(across(everything(), as.numeric)),\n  family = binomial(),\n  SL.library = sl_library,\n  cvControl = list(V = 5)\n)\n```\n:::\n\n\n## SuperLearner: Exposure Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npropensity_sl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nSuperLearner(Y = as.integer(nhefs_complete_uc$qsmk == \"Yes\"), X = mutate(select(nhefs_complete_uc,  \n    sex, race, age, education, smokeintensity, smokeyrs, exercise, active,  \n    wt71), across(everything(), as.numeric)), family = binomial(), SL.library = sl_library,  \n    cvControl = list(V = 5)) \n\n                   Risk       Coef\nSL.glm_All    0.1837871 0.00000000\nSL.ranger_All 0.1943978 0.05247478\nSL.gam_All    0.1825974 0.94752522\n```\n\n\n:::\n:::\n\n\n## SuperLearner: Outcome Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|2|3-5|\"}\noutcome_sl <- SuperLearner(\n  Y = nhefs_complete_uc$wt82_71,\n  X = nhefs_complete_uc |>\n    select(qsmk, sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n    mutate(across(everything(), as.numeric)),\n  family = gaussian(),\n  SL.library = sl_library,\n  cvControl = list(V = 5)\n)\n```\n:::\n\n\n## SuperLearner: Outcome Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noutcome_sl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nSuperLearner(Y = nhefs_complete_uc$wt82_71, X = mutate(select(nhefs_complete_uc,  \n    qsmk, sex, race, age, education, smokeintensity, smokeyrs, exercise,  \n    active, wt71), across(everything(), as.numeric)), family = gaussian(),  \n    SL.library = sl_library, cvControl = list(V = 5)) \n\n                  Risk       Coef\nSL.glm_All    55.41405 0.02228551\nSL.ranger_All 57.09105 0.15288964\nSL.gam_All    54.53583 0.82482486\n```\n\n\n:::\n:::\n\n\n## *Your Turn 1* {.small}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_b1ea7383\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n### First, create a character vector `sl_library` that specifies the following algorithms: \"SL.glm\", \"SL.ranger\", \"SL.gam\". Then, Fit a SuperLearner for the exposure model using the `SuperLearner` package. The predictors for this model should be the confounders identified in the DAG: `park_ticket_season`, `park_close`, and `park_temperature_high`. The outcome is `park_extra_magic_morning`.\n### Fit a SuperLearner for the outcome model using the `SuperLearner` package. The predictors for this model should be the confounders plus the exposure: `park_extra_magic_morning`, `park_ticket_season`, `park_close`, and `park_temperature_high`. The outcome is `wait_minutes_posted_avg`.\n### Inspect the fitted SuperLearner objects.\n\n## IPW with SuperLearner\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|1|3|5-9\"}\npropensity_scores <- predict(propensity_sl, type = \"response\")$pred[, 1]\n\nate_weights <- wt_ate(propensity_scores, nhefs_complete_uc$qsmk)\n\nipw_model <- lm(\n  wt82_71 ~ qsmk,\n  data = nhefs_complete_uc,\n  weights = ate_weights\n)\n```\n:::\n\n\n## IPW with SuperLearner\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(ipw_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     1.79     0.280      6.38 2.28e-10\n2 qsmkYes         3.31     0.407      8.14 8.30e-16\n```\n\n\n:::\n:::\n\n\n## G-computation with SuperLearner\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|1,5,7,11|13-14|16\"}\ndata_all_quit <- nhefs_complete_uc |>\n  select(qsmk, sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n  mutate(across(everything(), as.numeric)) |>\n  mutate(qsmk = 1)\n\ndata_all_no_quit <- nhefs_complete_uc |>\n  select(qsmk, sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n  mutate(across(everything(), as.numeric)) |>\n  mutate(qsmk = 0)\n\npred_quit <- predict(outcome_sl, newdata = data_all_quit)$pred[, 1]\npred_no_quit <- predict(outcome_sl, newdata = data_all_no_quit)$pred[, 1]\n\nmean(pred_quit - pred_no_quit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.912559\n```\n\n\n:::\n:::\n\n\n## *Your Turn 2*\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_9b540e88\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n### Implement the IPW algorithm using the SuperLearner propensity scores\n### Implement the G-computation algorithm using the SuperLearner outcome predictions\n\n# Targeted Maximum Likelihood Estimation (TMLE) {background-color=\"#23373B\"}\n\n## Targeted Learning\n- TMLE is a flexible, efficient method for estimating causal effects based in semi-parametric theory\n- TMLE solves three problems: doubly robustness, targeted estimation, and valid statistical inference\n\n## Targeted Learning: doubly robustness\n\n- In **IPW** and **G-computation**, we estimate the propensity score and outcome model separately. If either model is misspecified, the estimate will be biased.\n- In **TMLE**, we combine the two models in a way that is doubly robust: if either the propensity score or outcome model is correctly specified, the estimate will be consistent.\n\n## Targeted Learning: targeted estimation\n\n- In **IPW** and **G-computation**, we estimate the average treatment effect (ATE) using predictions from the exposure and outcome models. But these algorithms optimize for the predictions, not the ATE.\n- In **TMLE**, we adjust the predictions to specifically target the ATE. We change the bias-variance tradeoff to focus on the ATE rather than just minimizing prediction error. This is a debiasing step that also improves the efficiency of the estimate!\n\n## Targeted Learning: valid statistical inference\n- In **IPW** and **G-computation**, we cannot easily get valid confidence intervals with ML. Bootstrapping is often used, but it can be computationally intensive and not always valid.\n- In **TMLE**, we can use the influence curve to get valid confidence intervals. The influence curve is a way to estimate the variance of the TMLE estimate, even when using complex ML algorithms.\n\n## The TMLE Algorithm {background-color=\"#23373B\"}\n\n1. Start with SuperLearner predictions for the outcome\n2. Calculate the propensity scores using SuperLearner\n3. Create the clever covariate using the propensity scores\n\n## The TMLE Algorithm {background-color=\"#23373B\"}\n\n4. Fit the fluctuation model to learn how much to adjust the outcome predictions\n5. Update the predictions with the targeted adjustment\n6. Calculate the TMLE estimate and standard error using the influence curve\n\n## TMLE Step 1: Initial Predictions (on the bounded [0,1] scale)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# For TMLE with continuous outcomes, fit SuperLearner on bounded Y\nmin_y <- min(nhefs_complete_uc$wt82_71)\nmax_y <- max(nhefs_complete_uc$wt82_71)\ny_bounded <- (nhefs_complete_uc$wt82_71 - min_y) / (max_y - min_y)\n\n# Fit new SuperLearner on bounded outcome\noutcome_sl_bounded <- SuperLearner(\n  Y = y_bounded,\n  X = nhefs_complete_uc |>\n    select(qsmk, sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n    mutate(across(everything(), as.numeric)),\n  family = quasibinomial(),\n  SL.library = sl_library,\n  cvControl = list(V = 5)\n)\n```\n:::\n\n\n## TMLE Step 1: Initial Predictions (on the bounded [0,1] scale)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninitial_pred_quit <- predict(outcome_sl_bounded, newdata = data_all_quit)$pred[, 1]\ninitial_pred_no_quit <- predict(outcome_sl_bounded, newdata = data_all_no_quit)$pred[, 1]\n\n# Predictions for observed treatment\ninitial_pred_observed <- ifelse(\n  nhefs_complete_uc$qsmk == \"Yes\",\n  initial_pred_quit,\n  initial_pred_no_quit\n)\n```\n:::\n\n\n## TMLE Step 2: Clever Covariate\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|3-4\"}\nclever_covariate <- ifelse(\n  nhefs_complete_uc$qsmk == \"Yes\",\n  1 / propensity_scores,\n  -1 / (1 - propensity_scores)\n)\n```\n:::\n\n\n::: incremental\n- Not the same as IPW weights!\n- Part of the efficient influence function\n- Helps target the ATE specifically\n:::\n\n## TMLE Step 3: Targeting\n\n\n::: {.cell layout-align=\"center\" output-location='fragment'}\n\n```{.r .cell-code  code-line-numbers=\"|4|5|6|7|8|11-12\"}\n# Fluctuation model - learns how much to adjust\n# Use binomial family and work on logit scale\nfluctuation_model <- glm(\n  y_bounded ~\n     -1 +\n     offset(qlogis(initial_pred_observed)) +\n     clever_covariate,\n  family = quasibinomial()\n)\n\nepsilon <- coef(fluctuation_model)\nepsilon\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclever_covariate \n     0.003466991 \n```\n\n\n:::\n:::\n\n\n::: incremental\n- Small epsilon = initial estimate was good\n- Large epsilon = needed more adjustment\n:::\n\n## TMLE Step 4: Update Predictions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|2-3|6-7\"}\n# Update predictions on logit scale, then transform back\nlogit_pred_quit <- qlogis(initial_pred_quit) + epsilon * (1 / propensity_scores)\nlogit_pred_no_quit <- qlogis(initial_pred_no_quit) + epsilon * (-1 / (1 - propensity_scores))\n\n# Transform back to probability scale\ntargeted_pred_quit <- plogis(logit_pred_quit)\ntargeted_pred_no_quit <- plogis(logit_pred_no_quit)\n```\n:::\n\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-bonus-ml-for-causal_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## *Your Turn 3*\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_78fb48f8\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n### Calculate initial predictions for treated/control scenarios\n### Create the clever covariate using propensity scores\n### Fit the fluctuation model with offset and no intercept\n### Update predictions with the targeted adjustment\n\n## TMLE ATE\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninitial_ate <- mean(\n  initial_pred_quit - initial_pred_no_quit\n  # Transform back to original scale for ATE\n) * (max_y - min_y)\n\ntargeted_ate <- mean(\n  targeted_pred_quit - targeted_pred_no_quit\n) * (max_y - min_y)\n\ntibble(initial = initial_ate, targeted = targeted_ate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  initial targeted\n    <dbl>    <dbl>\n1    2.69     3.16\n```\n\n\n:::\n:::\n\n\n## TMLE Inference\n\n\n::: {.cell layout-align=\"center\" output-location='slide'}\n\n```{.r .cell-code}\ntargeted_pred_observed <- ifelse(\n  nhefs_complete_uc$qsmk == \"Yes\",\n  targeted_pred_quit,\n  targeted_pred_no_quit\n)\n\n# IC uses bounded outcomes and predictions\nic <- clever_covariate * (y_bounded - targeted_pred_observed) +\n      targeted_pred_quit - targeted_pred_no_quit - targeted_ate / (max_y - min_y)\n\n# Standard error on original scale\nse_tmle <- sqrt(var(ic) / nrow(nhefs_complete_uc)) * (max_y - min_y)\n\n# 95% CI\ntibble(\n  ate = targeted_ate,\n  se = se_tmle,\n  lower_ci = targeted_ate - 1.96 * se_tmle,\n  upper_ci = targeted_ate + 1.96 * se_tmle\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n    ate    se lower_ci upper_ci\n  <dbl> <dbl>    <dbl>    <dbl>\n1  3.16 0.444     2.29     4.03\n```\n\n\n:::\n:::\n\n\n## Using the tmle Package\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tmle)\n\ntmle_result <- tmle(\n  Y = nhefs_complete_uc$wt82_71,\n  A = as.integer(nhefs_complete_uc$qsmk == \"Yes\"),\n  W = nhefs_complete_uc |>\n    select(sex, race, age, education, smokeintensity,\n           smokeyrs, exercise, active, wt71) |>\n    mutate(across(everything(), as.numeric)),\n  Q.SL.library = sl_library,\n  g.SL.library = sl_library\n)\n\ntibble(\n  ate = tmle_result$estimates$ATE$psi,\n  lower_ci = tmle_result$estimates$ATE$CI[[1]],\n  upper_ci = tmle_result$estimates$ATE$CI[[2]]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n    ate lower_ci upper_ci\n  <dbl>    <dbl>    <dbl>\n1  3.48     2.55     4.41\n```\n\n\n:::\n:::\n\n\n\n## *Your Turn 4*\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_3cef0ad8\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n### Calculate the TMLE ATE and compare to the initial (g-computation) estimate\n### Work through the code to compute the variance and CIs (nothing to change here)\n\n## Key Takeaways\n\n- ML improves flexibility for confounding functional form, not identification\n- Still need DAGs and causal assumptions\n- TMLE is statistically efficient, updates predictions to target the causal effect\n- Valid inference even with complex ML\n\n## Resources {background-color=\"#23373B\"}\n\n### *Targeted Learning* by Mark van der Laan and Sherri Rose (THE book... see the sequel for longitudinal problems)\n### [*Introduction to Modern Causal Inference*](https://alejandroschuler.github.io/mci/introduction-to-modern-causal-inference.html) by Alejandro Schuler and Mark van der Laan (Great introduction to the math and theory)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"15-bonus-ml-for-causal_files/libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"15-bonus-ml-for-causal_files/libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}